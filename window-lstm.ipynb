{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.metrics import r2_score \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data\n",
    "\n",
    "The following method takes input the dataset name and does the follow two things:\n",
    "\n",
    "## Discretising Data\n",
    "To reduce the computational load of the problem, we have only taken data at every 1% strain step instead of all timesteps for which the FE Simulation runs.\n",
    "\n",
    "- `OUT_FILE_PATH`: Location of the `out.csv` for a dataset, which is output at the end of FE simulations\n",
    "- `DATA_FILE_PATH`: Data file generated using the seacas-exodus python script\n",
    "- `out_file[\"eff_strain\"]` - contains all values of applied strain at every timestep, we look for timesteps at which there is 1\\% increment in strain and record them. \n",
    "- `df` - contains all the data as read from the FE output file.\n",
    "- `df_steps` - contains the data only at 1\\% strain steps \n",
    "- `df_steps_norm` - contains the normalised data only at 1\\% strain steps\n",
    "\n",
    "## Grouping Data\n",
    "The method returns a pandas grouped object with data grouped with respect to x and y coordinates. So all the variable values with the same x and y are present together.\n",
    "\n",
    "- `df_group`: Pandas grouped object returned by the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(dataset):\n",
    "    OUT_FILE_PATH = \"/home/sarthak/projects/seacas-exodus/lib/soudip_dataset/%s/out.csv\" % dataset\n",
    "    DATA_FILE_PATH = \"/home/sarthak/projects/model_training/new_data/%s.csv\" % dataset\n",
    "    print(OUT_FILE_PATH)\n",
    "    print(DATA_FILE_PATH)\n",
    "\n",
    "    out_file = pd.read_csv(OUT_FILE_PATH)\n",
    "    lim = 0.00\n",
    "    timesteps = []\n",
    "    time = []\n",
    "    for i in range(len(out_file)):\n",
    "        if(out_file[\"eff_strain\"][i] >= lim):\n",
    "            timesteps.append(i+1)\n",
    "            time.append(out_file[\"time\"][i])\n",
    "            lim = lim + 0.01\n",
    "\n",
    "    print(\"Reading %s \\n\" % dataset)\n",
    "    # Reading data\n",
    "    df = pd.read_csv(DATA_FILE_PATH)\n",
    "    df_dropped = df.drop(columns=['strain_yy', 'phases', 'pressure', 'sdv22', 'sdv23',\n",
    "                                  'total_strain_xy', 'elem_id', 'blk_id', 'total_stress_xx', 'total_stress_yy',\n",
    "                                  'total_strain_xx', 'total_strain_yy' ])\n",
    "    df_norm = (df_dropped-df_dropped.min())/(df_dropped.max()-df_dropped.min())\n",
    "    df_norm.tail()\n",
    "    scale_steps = int(np.ceil(len(out_file)/(len(df_dropped)/160000)))\n",
    "    steps = np.ceil((np.array(timesteps)/scale_steps))\n",
    "    df_steps = df_dropped[df_dropped[\"time\"].isin(steps)] \n",
    "    df_steps_norm = (df_steps-df_steps.min())/(df_steps.max()-df_steps.min())\n",
    "    df_steps_norm = df_steps_norm.drop(columns=['time'])\n",
    "    df_group = df_steps_norm.groupby([\"elem_x\", \"elem_y\"])\n",
    "    return df_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structuring Data\n",
    "\n",
    "`series_to_supervised`: takes in a sequence, the number of steps to be used as inputs and the number of steps in output. It restructures a series and return it in a format for supervised learning problem. The function returns two arrays, one is the input (x) and the other one is output associated with each input.\n",
    "\n",
    "`structure_data`: takes the pandas grouped object returned by the `read_data` function and structures it for training. Our model is designed such that it takes two steps and predicts the next 13 steps using the data\n",
    "\n",
    "| Input | Output |\n",
    "| :---: | :-----: |\n",
    "| X<sub>1</sub>, X<sub>2</sub> | X<sub>3</sub> ..... X<sub>15</sub> |\n",
    "| X<sub>2</sub>, X<sub>3</sub> | X<sub>4</sub> ..... X<sub>16</sub> |\n",
    "| . | . |\n",
    "| . | . |\n",
    "| . | . |\n",
    "\n",
    "Each X<sub>n</sub> is of the shape (samples, timesteps, variables) and are 3 dimensional tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(sequences, n_steps_in, n_steps_out, dropnan=True):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out\n",
    "        # check if we beyond the dataset\n",
    "        if out_end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix: out_end_ix, :]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def structure_data(df_group):\n",
    "    x_raw_list = []\n",
    "    y_raw_list = []\n",
    "    for name, group in df_group:\n",
    "        strain_vals = group['eff_strain'].values\n",
    "        stress_vals = group['vonmises'].values\n",
    "        tri_vals = group['triaxiality'].values\n",
    "        all_vals = np.stack((strain_vals, stress_vals, tri_vals), axis = 1 )\n",
    "        x, y = series_to_supervised(all_vals, 2, 13)\n",
    "        x_raw_list.append(x)\n",
    "        y_raw_list.append(y)\n",
    "    x_vals = np.concatenate(x_raw_list)\n",
    "    y_vals = np.concatenate(y_raw_list)\n",
    "    return x_vals, y_vals, x_raw_list, y_raw_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sarthak/projects/seacas-exodus/lib/soudip_dataset/AR1/out.csv\n",
      "/home/sarthak/projects/model_training/new_data/AR1.csv\n",
      "Reading AR1 \n",
      "\n",
      "/home/sarthak/projects/seacas-exodus/lib/soudip_dataset/test61/out.csv\n",
      "/home/sarthak/projects/model_training/new_data/test61.csv\n",
      "Reading test61 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_group = read_data('AR1')\n",
    "x_vals, y_vals, x_raw_list, y_raw_list = structure_data(df_group)\n",
    "\n",
    "df_group_2 = read_data('test61')\n",
    "x_vals_2, y_vals_2, x_raw_list_2, y_raw_list_2 = structure_data(df_group_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1280000, 2, 3)\n",
      "(1280000, 13, 3)\n"
     ]
    }
   ],
   "source": [
    "# Put together data from different datasets for training\n",
    "x_vals = np.concatenate((x_vals, x_vals_2))\n",
    "y_vals = np.concatenate((y_vals, y_vals_2))\n",
    "#n_steps_in, n_steps_out = 1, 1\n",
    "print(x_vals.shape)\n",
    "print(y_vals.shape)\n",
    "n = x_vals.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# Splitting data into train, test and validation data\n",
    "train_x = x_vals[0:int(0.7*n), :, :]\n",
    "train_y = y_vals[0:int(0.7*n), :, :]\n",
    "test_x = x_vals[int(0.7*n):int(0.9*n), :, :]\n",
    "test_y = y_vals[int(0.7*n):int(0.9*n), :, :]\n",
    "val_x = x_vals[int(0.9*n):, :, :]\n",
    "val_y = y_vals[int(0.9*n):, :, :]\n",
    "print(train_x.shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the model\n",
    "The model along with all the hyperparameters is defined below. In the below example the hyper parameters are as follows:\n",
    "\n",
    "- Number of layers: 8\n",
    "- Number of LSTM unit in each layer: 100\n",
    "- Activation function: `relu`\n",
    "- Optimisation function: `adam`\n",
    "- Loss function: `mse`\n",
    "- Epochs: 50\n",
    "- `return_sequences`: is given as `True` because we require he output of the LSTM at each timestep and not just the last one. \n",
    "- `history`: stores information about the training process of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "28000/28000 [==============================] - 732s 26ms/step - loss: 7.3230e-04\n",
      "Epoch 2/50\n",
      "28000/28000 [==============================] - 1050s 37ms/step - loss: 2.2486e-04\n",
      "Epoch 3/50\n",
      "28000/28000 [==============================] - 696s 25ms/step - loss: 0.0036\n",
      "Epoch 4/50\n",
      "28000/28000 [==============================] - 679s 24ms/step - loss: 1.6257e-04\n",
      "Epoch 5/50\n",
      "28000/28000 [==============================] - 678s 24ms/step - loss: 1.5410e-04\n",
      "Epoch 6/50\n",
      "28000/28000 [==============================] - 680s 24ms/step - loss: 1.5304e-04\n",
      "Epoch 7/50\n",
      "28000/28000 [==============================] - 679s 24ms/step - loss: 1.4601e-04\n",
      "Epoch 8/50\n",
      "28000/28000 [==============================] - 677s 24ms/step - loss: 1.3995e-04\n",
      "Epoch 9/50\n",
      "28000/28000 [==============================] - 681s 24ms/step - loss: 1.3858e-04\n",
      "Epoch 10/50\n",
      "28000/28000 [==============================] - 672s 24ms/step - loss: 1.3581e-04\n",
      "Epoch 11/50\n",
      "28000/28000 [==============================] - 675s 24ms/step - loss: 3.1628e-04\n",
      "Epoch 12/50\n",
      "28000/28000 [==============================] - 675s 24ms/step - loss: 1.3708e-04\n",
      "Epoch 13/50\n",
      "28000/28000 [==============================] - 675s 24ms/step - loss: 1.3346e-04\n",
      "Epoch 14/50\n",
      "28000/28000 [==============================] - 677s 24ms/step - loss: 1.3109e-04\n",
      "Epoch 15/50\n",
      "28000/28000 [==============================] - 678s 24ms/step - loss: 1.4403e-04\n",
      "Epoch 16/50\n",
      "28000/28000 [==============================] - 681s 24ms/step - loss: 1.3332e-04\n",
      "Epoch 17/50\n",
      "28000/28000 [==============================] - 681s 24ms/step - loss: 1.3011e-04\n",
      "Epoch 18/50\n",
      "28000/28000 [==============================] - 681s 24ms/step - loss: 1.2911e-04\n",
      "Epoch 19/50\n",
      "28000/28000 [==============================] - 680s 24ms/step - loss: 1.2692e-04\n",
      "Epoch 20/50\n",
      "28000/28000 [==============================] - 680s 24ms/step - loss: 1.2609e-04\n",
      "Epoch 21/50\n",
      "28000/28000 [==============================] - 674s 24ms/step - loss: 1.2779e-04\n",
      "Epoch 22/50\n",
      "28000/28000 [==============================] - 677s 24ms/step - loss: 1.2863e-04\n",
      "Epoch 23/50\n",
      "28000/28000 [==============================] - 677s 24ms/step - loss: 0.0068\n",
      "Epoch 24/50\n",
      "28000/28000 [==============================] - 678s 24ms/step - loss: 0.8488\n",
      "Epoch 25/50\n",
      "28000/28000 [==============================] - 679s 24ms/step - loss: 1.2835e-04\n",
      "Epoch 26/50\n",
      "28000/28000 [==============================] - 679s 24ms/step - loss: 1.2819e-04\n",
      "Epoch 27/50\n",
      "28000/28000 [==============================] - 677s 24ms/step - loss: 1.2640e-04\n",
      "Epoch 28/50\n",
      "28000/28000 [==============================] - 678s 24ms/step - loss: 1.2417e-04\n",
      "Epoch 29/50\n",
      "28000/28000 [==============================] - 678s 24ms/step - loss: 1.2302e-04\n",
      "Epoch 30/50\n",
      "28000/28000 [==============================] - 677s 24ms/step - loss: 1.2732e-04\n",
      "Epoch 31/50\n",
      "28000/28000 [==============================] - 679s 24ms/step - loss: 1.2937e-04\n",
      "Epoch 32/50\n",
      "28000/28000 [==============================] - 679s 24ms/step - loss: 0.0031\n",
      "Epoch 33/50\n",
      "28000/28000 [==============================] - 679s 24ms/step - loss: 1.3170e-04\n",
      "Epoch 34/50\n",
      "28000/28000 [==============================] - 679s 24ms/step - loss: 1.5654e-04\n",
      "Epoch 35/50\n",
      "28000/28000 [==============================] - 689s 25ms/step - loss: 1.3406e-04\n",
      "Epoch 36/50\n",
      "28000/28000 [==============================] - 685s 24ms/step - loss: 1.4824e-04\n",
      "Epoch 37/50\n",
      "28000/28000 [==============================] - 681s 24ms/step - loss: 1.2569e-04\n",
      "Epoch 38/50\n",
      "28000/28000 [==============================] - 679s 24ms/step - loss: 1.2350e-04\n",
      "Epoch 39/50\n",
      "28000/28000 [==============================] - 676s 24ms/step - loss: 1.2173e-04\n",
      "Epoch 40/50\n",
      "28000/28000 [==============================] - 675s 24ms/step - loss: 1.2075e-04\n",
      "Epoch 41/50\n",
      "28000/28000 [==============================] - 676s 24ms/step - loss: 1.1976e-04\n",
      "Epoch 42/50\n",
      "28000/28000 [==============================] - 672s 24ms/step - loss: 1.2494e-04\n",
      "Epoch 43/50\n",
      "28000/28000 [==============================] - 675s 24ms/step - loss: 1.1983e-04\n",
      "Epoch 44/50\n",
      "28000/28000 [==============================] - 672s 24ms/step - loss: 1.1830e-04\n",
      "Epoch 45/50\n",
      "28000/28000 [==============================] - 673s 24ms/step - loss: 1.1764e-04\n",
      "Epoch 46/50\n",
      "28000/28000 [==============================] - 673s 24ms/step - loss: 31.5270\n",
      "Epoch 47/50\n",
      "28000/28000 [==============================] - 674s 24ms/step - loss: 5.0949\n",
      "Epoch 48/50\n",
      "28000/28000 [==============================] - 673s 24ms/step - loss: 0.0135\n",
      "Epoch 49/50\n",
      "28000/28000 [==============================] - 674s 24ms/step - loss: 1.3973e-04\n",
      "Epoch 50/50\n",
      "28000/28000 [==============================] - 674s 24ms/step - loss: 1.2714e-04\n"
     ]
    }
   ],
   "source": [
    "def get_compiled_model():\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.LSTM(100, activation='relu', input_shape=(2, 3)))\n",
    "    model.add(tf.keras.layers.RepeatVector(13))\n",
    "    model.add(tf.keras.layers.LSTM(100, activation='relu', return_sequences=True))\n",
    "    model.add(tf.keras.layers.LSTM(100, activation='relu', return_sequences=True))\n",
    "    model.add(tf.keras.layers.LSTM(100, activation='relu', return_sequences=True))\n",
    "    model.add(tf.keras.layers.LSTM(100, activation='relu', return_sequences=True))\n",
    "    model.add(tf.keras.layers.LSTM(100, activation='relu', return_sequences=True))\n",
    "    model.add(tf.keras.layers.LSTM(100, activation='relu', return_sequences=True))\n",
    "    model.add(tf.keras.layers.LSTM(100, activation='relu', return_sequences=True))\n",
    "    model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(train_x.shape[2])))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "model = get_compiled_model()\n",
    "history = model.fit(train_x, train_y, epochs=50, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the model\n",
    "The trained model is saved in the directory created above for future analysis and making predictions. A brief summary of the model architecture is also stored in a text file called `model_summary.txt`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 47s 12ms/step - loss: 3.4473e-04\n",
      "0.0003447342023719102\n",
      "WARNING:tensorflow:From /home/sarthak/.local/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: /home/sarthak/projects/model_training/trained_models/window_lstm_AR1_61/the_model/assets\n"
     ]
    }
   ],
   "source": [
    "print(model.evaluate(val_x, val_y))\n",
    "base_path = '/home/sarthak/projects/model_training/trained_models/window_lstm_AR1_61'\n",
    "model.save(base_path+'/the_model')\n",
    "with open(base_path + '/model_summary.txt','w') as fh:\n",
    "    # Pass the file handle in as a lambda function to make it callable\n",
    "    model.summary(print_fn=lambda x: fh.write(x + '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Predictions\n",
    "\n",
    "The first part of this functions is similar to the `read_data` function. It loads the data for a new microsructure and structures it in the format used by the model. Then the model uses the first two time steps to predict 13 times steps into the future. The predictions are compared with the true values and the Mean Absolute Error and R2 values are calculated for the final time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(dataset, start, stop, model):\n",
    "    OUT_FILE_PATH = \"/home/sarthak/projects/seacas-exodus/lib/soudip_dataset/%s/out.csv\" % dataset\n",
    "    DATA_FILE_PATH = \"/home/sarthak/projects/model_training/new_data/%s.csv\" % dataset\n",
    "    print(OUT_FILE_PATH)\n",
    "    print(DATA_FILE_PATH)\n",
    "\n",
    "    out_file = pd.read_csv(OUT_FILE_PATH)\n",
    "    lim = 0.00\n",
    "    timesteps = []\n",
    "    time = []\n",
    "    for i in range(len(out_file)):\n",
    "        if(out_file[\"eff_strain\"][i] >= lim):\n",
    "            timesteps.append(i+1)\n",
    "            time.append(out_file[\"time\"][i])\n",
    "            lim = lim + 0.01\n",
    "\n",
    "    print(\"Reading %s \\n\" % dataset)\n",
    "    # Reading data\n",
    "    df = pd.read_csv(DATA_FILE_PATH)\n",
    "    df_dropped = df.drop(columns=['strain_yy', 'phases', 'pressure', 'sdv22', 'sdv23',\n",
    "                                  'total_strain_xy', 'elem_id', 'blk_id', 'total_stress_xx', 'total_stress_yy',\n",
    "                                  'total_strain_xx', 'total_strain_yy' ])\n",
    "    df_norm = (df_dropped-df_dropped.min())/(df_dropped.max()-df_dropped.min())\n",
    "    df_norm.tail()\n",
    "    scale_steps = int(np.ceil(len(out_file)/(len(df_dropped)/160000)))\n",
    "    steps = np.ceil((np.array(timesteps)/scale_steps))\n",
    "    df_steps = df_dropped[df_dropped[\"time\"].isin(steps)] \n",
    "    df_steps_norm = (df_steps-df_steps.min())/(df_steps.max()-df_steps.min())\n",
    "    df_steps_norm = df_steps_norm.drop(columns=['time'])\n",
    "    df_group = df_steps_norm.groupby([\"elem_x\", \"elem_y\"])\n",
    "    \n",
    "    x_vals, y_vals, x_raw_list, y_raw_list = structure_data(df_group)\n",
    "    #df_group = df_group[[\"eff_strain\", \"vonmises\", \"triaxiality\"]]\n",
    "    #first_step = df_group.nth(start).values.reshape(160000, 1, 3)\n",
    "    input_step = np.array(x_raw_list)[:,start,:,:]\n",
    "    #next_step = model.predict(first_step)\n",
    "    next_step = model.predict(input_step)\n",
    "    input_step = np.concatenate(((np.delete(input_step, 0, 1)), next_step), axis = 1)\n",
    "    true_val = np.array(y_raw_list)\n",
    "    ans = true_val[:,0,:,:]\n",
    "    variables = [\"eff_strain\", \"vonmises\", \"triaxiality\"]\n",
    "    for j in range(0,3):\n",
    "        var_name = variables[j]\n",
    "        true_scaled = (ans[:,stop,j]*(df_steps[var_name].max() - df_steps[var_name].min())) + df_steps[var_name].min()\n",
    "        pred_scaled = (next_step[:,stop,j]*(df_steps[var_name].max() - df_steps[var_name].min())) + df_steps[var_name].min()\n",
    "        print(r2_score(true_scaled, pred_scaled))\n",
    "        print(np.sum(abs(true_scaled - pred_scaled))/160000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sarthak/projects/seacas-exodus/lib/soudip_dataset/test65/out.csv\n",
      "/home/sarthak/projects/model_training/new_data/test65.csv\n",
      "Reading test65 \n",
      "\n",
      "0.8789629454249992\n",
      "0.017838955827673208\n",
      "0.9949015731208434\n",
      "26.226886561781676\n",
      "0.8993939228554485\n",
      "0.08973756983621067\n"
     ]
    }
   ],
   "source": [
    "make_prediction('test65', 0, 12, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
